{"block_file": {"custom/make_prediction.py:custom:python:make prediction": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nimport json\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom concurrent import futures\nfrom google.cloud import pubsub_v1\nfrom typing import Callable\nimport os\nfrom os import path\nfrom concurrent.futures import TimeoutError\nfrom mlflow import MlflowClient\n\n#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credentials.json'\n#os.environ['GCP_PROJECT_NAME'] = 'ibphilippov-mlops' #!!!! \u0443\u0434\u0430\u043b\u0438\u0442\u044c\n#os.environ['ARTIFACT_STORAGE'] = 'ibphilippov-mlops-storage'\n\ndef get_callback(\n    publish_future: pubsub_v1.publisher.futures.Future, data: str\n) -> Callable[[pubsub_v1.publisher.futures.Future], None]:\n    def callback(publish_future: pubsub_v1.publisher.futures.Future) -> None:\n        try:\n            # Wait 60 seconds for the publish call to succeed.\n            print(publish_future.result(timeout=60))\n        except futures.TimeoutError:\n            print(f\"Publishing {data} timed out.\")\n\n    return callback\n\ndef callback_listen(message: pubsub_v1.subscriber.message.Message) -> None:\n    global processed_data\n    print(f\"Received {message}.\")\n    processed_data.append(json.loads(message.data.decode('utf-8')))\n    message.ack()\n    return processed_data\n\ndef listen(message_sent):\n    global processed_data\n    project_id=os.environ['GCP_PROJECT_NAME']\n    timeout = 15\n    subscriber = pubsub_v1.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project_id, 'read')\n    processed_data=[]\n    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback_listen)\n    print(f\"Listening for messages on {subscription_path}..\\n\")\n\n    # Wrap subscriber in a 'with' block to automatically call close() when done.\n    with subscriber:\n        try:\n            streaming_pull_future.result(timeout=timeout)\n        except TimeoutError:\n            streaming_pull_future.cancel()  # Trigger the shutdown.\n            streaming_pull_future.result() \n    for message in processed_data:\n        if 1==1:#message['data']==message_sent['event']:\n            return message['prediction'], message['prepared_X']\n\ndef publish(data):\n    project_id=os.environ['GCP_PROJECT_NAME']\n    topic_id = \"predictions-input\"\n    publisher = pubsub_v1.PublisherClient()\n    topic_path = publisher.topic_path(project_id, topic_id)\n    publish_futures = []\n    data = json.dumps(data).encode('utf-8')\n    publish_future = publisher.publish(topic_path, data)\n    # Non-blocking. Publish failures are handled in the callback function.\n    publish_future.add_done_callback(get_callback(publish_future, data))\n    publish_futures.append(publish_future)\n\n    # Wait for all the publish futures to resolve before exiting.\n    futures.wait(publish_futures, return_when=futures.ALL_COMPLETED)\n\n    print(f\"Published messages with error handler to {topic_path}.\")\n\n\n@custom\ndef get_prediction(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n\n    project_id=os.environ['GCP_PROJECT_NAME']\n    bucket_name=os.environ['ARTIFACT_STORAGE']\n\n    event_data = kwargs.get('event_data')\n    if event_data is None:\n        event_data =  [{\"h_three_points_pct\": 10.8, \"h_three_points_att\": 99.0, \"h_two_points_pct\": 41.0, \"h_two_points_att\": 39.0, \"h_offensive_rebounds\": 5.0, \"h_defensive_rebounds\": 30.0, \"h_turnovers\": 9.0, \"h_steals\": 0.0, \"h_blocks\": 4.0, \"h_personal_fouls\": 15.0, \"h_points\": 70.0, \"a_three_points_pct\": 41.7, \"a_three_points_att\": 12.0, \"a_two_points_pct\": 46.3, \"a_two_points_att\": 54.0, \"a_offensive_rebounds\": 5.0, \"a_defensive_rebounds\": 27.0, \"a_turnovers\": 10.0, \"a_steals\": 2.0, \"a_blocks\": 5.0, \"a_personal_fouls\": 17.0, \"a_points\": 73.0},{\"h_three_points_pct\": 34.8, \"h_three_points_att\": 23.0, \"h_two_points_pct\": 41.0, \"h_two_points_att\": 39.0, \"h_offensive_rebounds\": 5.0, \"h_defensive_rebounds\": 30.0, \"h_turnovers\": 9.0, \"h_steals\": 0.0, \"h_blocks\": 4.0, \"h_personal_fouls\": 15.0, \"h_points\": 70.0, \"a_three_points_pct\": 41.7, \"a_three_points_att\": 12.0, \"a_two_points_pct\": 46.3, \"a_two_points_att\": 54.0, \"a_offensive_rebounds\": 5.0, \"a_defensive_rebounds\": 27.0, \"a_turnovers\": 10.0, \"a_steals\": 2.0, \"a_blocks\": 5.0, \"a_personal_fouls\": 17.0, \"a_points\": 73.0}]\n    if type(event_data)==str:\n        event_data = json.loads(event_data)\n    \n\n    client = MlflowClient(tracking_uri=\"http://mlflow:5000\")\n    model_name = \"basketball_predictor\"\n    latest_versions = client.get_latest_versions(name=model_name)\n\n    for version in latest_versions:\n        logged_model=version.source[:-5]\n    print(logged_model)\n\n    message={\"event\":event_data,\"logged_model\":logged_model,\"project_id\": project_id,\"bucket_name\": bucket_name}\n    publish(message)\n    prediction,features=listen(message)\n    print(prediction)\n    print({\"features\":features, \"logged_model\":logged_model, \"prediction\": prediction })\n    return {\"features\":features, \"logged_model\":logged_model, \"prediction\": prediction }\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/make_prediction.py", "language": "python", "type": "custom", "uuid": "make_prediction"}, "custom/prepare_current_data.py:custom:python:prepare current data": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport pandas as pd\nimport evidently\n\n@custom\ndef transform_custom(*args, **kwargs):\n    #data = kwargs.get('data')\n    data = {'features': [{'h_three_points_pct': 0.10800000000000001, 'h_three_points_att': 99.0, 'h_two_points_pct': 0.41, 'h_two_points_att': 39.0, 'h_offensive_rebounds': 5.0, 'h_defensive_rebounds': 30.0, 'h_turnovers': 9.0, 'h_steals': 0.0, 'h_blocks': 4.0, 'h_personal_fouls': 15.0, 'h_points': 70.0, 'a_three_points_pct': 0.41700000000000004, 'a_three_points_att': 12.0, 'a_two_points_pct': 0.46299999999999997, 'a_two_points_att': 54.0, 'a_offensive_rebounds': 5.0, 'a_defensive_rebounds': 27.0, 'a_turnovers': 10.0, 'a_steals': 2.0, 'a_blocks': 5.0, 'a_personal_fouls': 17.0, 'a_points': 73.0}, {'h_three_points_pct': 0.348, 'h_three_points_att': 23.0, 'h_two_points_pct': 0.41, 'h_two_points_att': 39.0, 'h_offensive_rebounds': 5.0, 'h_defensive_rebounds': 30.0, 'h_turnovers': 9.0, 'h_steals': 0.0, 'h_blocks': 4.0, 'h_personal_fouls': 15.0, 'h_points': 70.0, 'a_three_points_pct': 0.41700000000000004, 'a_three_points_att': 12.0, 'a_two_points_pct': 0.46299999999999997, 'a_two_points_att': 54.0, 'a_offensive_rebounds': 5.0, 'a_defensive_rebounds': 27.0, 'a_turnovers': 10.0, 'a_steals': 2.0, 'a_blocks': 5.0, 'a_personal_fouls': 17.0, 'a_points': 73.0}], 'logged_model': 'gs://ibphilippov-mlops-storage/artifacts/719fb9ce3cf143408d7e63298521794e/artifacts/', 'prediction': [8.340391221695402, 9.876255483351072]}\n    event_data = pd.DataFrame.from_records(data['features'])\n    event_data = event_data[['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','h_points','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls','a_points']]\n    if type(data['prediction'])==dict:\n        event_pred = pd.DataFrame([data['prediction']],columns=['prediction'])\n    else:\n        event_pred = pd.DataFrame(data['prediction'],columns=['prediction'])\n    event_data=event_data.join(event_pred)\n    if 'h_points' in event_data.columns and 'a_points' in event_data.columns:\n        event_data['target']=event_data['h_points']-event_data['a_points']\n        event_data = event_data.drop(columns=['h_points','a_points'])\n    logged_model=data['logged_model']\n    return event_data, logged_model\n\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/prepare_current_data.py", "language": "python", "type": "custom", "uuid": "prepare_current_data"}, "custom/set_models.py:custom:python:set models": {"content": "from typing import Dict, List, Tuple\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef models(*args, **kwargs) -> Tuple[List[str], List[Dict[str, str]]]:\n    \"\"\"\n    models: comma separated strings\n        linear_model.Lasso\n        linear_model.LinearRegression\n        svm.LinearSVR\n        ensemble.ExtraTreesRegressor\n        ensemble.GradientBoostingRegressor\n        ensemble.RandomForestRegressor\n    \"\"\"\n    model_names: str = kwargs.get(\n        'models', 'svm.LinearSVR,ensemble.GradientBoostingRegressor,linear_model.LinearRegression,linear_model.Lasso,ensemble.RandomForestRegressor'\n    )\n    child_data: List[str] = [\n        model_name.strip() for model_name in model_names.split(',')\n    ]\n    child_metadata: List[Dict] = [\n        dict(block_uuid=model_name.split('.')[-1]) for model_name in child_data\n    ]\n\n    return child_data, child_metadata\n", "file_path": "custom/set_models.py", "language": "python", "type": "custom", "uuid": "set_models"}, "custom/starry_elm.py:custom:python:starry elm": {"content": "\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nimport json\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom concurrent import futures\nfrom google.cloud import pubsub_v1\nfrom typing import Callable\nimport os\nfrom os import path\nfrom concurrent.futures import TimeoutError\nfrom mlflow import MlflowClient\n\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credentials.json'\nos.environ['GCP_PROJECT_NAME'] = 'ibphilippov-mlops' #!!!! \u0443\u0434\u0430\u043b\u0438\u0442\u044c\ndef define_query(year):\n    query = f'''SELECT h_three_points_pct, h_three_points_att, h_two_points_pct, h_two_points_att, h_offensive_rebounds, h_defensive_rebounds, h_turnovers, h_steals, h_blocks, h_personal_fouls, h_points,\n    a_three_points_pct, a_three_points_att, a_two_points_pct, a_two_points_att, a_offensive_rebounds, a_defensive_rebounds, a_turnovers, a_steals, a_blocks, a_personal_fouls, a_points\n    FROM bigquery-public-data.ncaa_basketball.mbb_games_sr WHERE EXTRACT(year FROM gametime)={year}'''\n    return query\n\ndef get_callback(\n    publish_future: pubsub_v1.publisher.futures.Future, data: str\n) -> Callable[[pubsub_v1.publisher.futures.Future], None]:\n    def callback(publish_future: pubsub_v1.publisher.futures.Future) -> None:\n        try:\n            # Wait 60 seconds for the publish call to succeed.\n            print(publish_future.result(timeout=60))\n        except futures.TimeoutError:\n            print(f\"Publishing {data} timed out.\")\n\n    return callback\n\ndef callback_listen(message: pubsub_v1.subscriber.message.Message) -> None:\n    global processed_data\n    print(f\"Received {message}.\")\n    processed_data.append(json.loads(message.data.decode('utf-8')))\n    message.ack()\n    return processed_data\n\ndef listen(message_sent):\n    global processed_data\n    project_id=os.environ['GCP_PROJECT_NAME']\n    timeout = 15\n    subscriber = pubsub_v1.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project_id, 'read')\n    processed_data=[]\n    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback_listen)\n    print(f\"Listening for messages on {subscription_path}..\\n\")\n\n    # Wrap subscriber in a 'with' block to automatically call close() when done.\n    with subscriber:\n        try:\n            streaming_pull_future.result(timeout=timeout)\n        except TimeoutError:\n            streaming_pull_future.cancel()  # Trigger the shutdown.\n            streaming_pull_future.result() \n    for message in processed_data:\n        if 1==1:#message['data']==message_sent['event']:\n            return message['prediction']\n\ndef publish(data):\n    project_id=os.environ['GCP_PROJECT_NAME']\n    topic_id = \"predictions-input\"\n    publisher = pubsub_v1.PublisherClient()\n    topic_path = publisher.topic_path(project_id, topic_id)\n    publish_futures = []\n    data = json.dumps(data).encode('utf-8')\n    publish_future = publisher.publish(topic_path, data)\n    # Non-blocking. Publish failures are handled in the callback function.\n    publish_future.add_done_callback(get_callback(publish_future, data))\n    publish_futures.append(publish_future)\n\n    # Wait for all the publish futures to resolve before exiting.\n    futures.wait(publish_futures, return_when=futures.ALL_COMPLETED)\n\n    print(f\"Published messages with error handler to {topic_path}.\")\n\n\n@data_exporter\n\ndef make_prediction(*args, **kwargs):\n    print('start')\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    #train_year=2013\n    #val_year=2015\n\n    #config_path = path.join(get_repo_path(), 'io_config.yaml')\n    #config_profile = 'default'\n    #query=define_query(val_year)\n    #val_data=BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n    #X=val_data[['h_two_points_pct','h_three_points_pct','h_steals','h_blocks','h_personal_fouls','a_two_points_pct','a_three_points_pct','a_steals','a_blocks','a_personal_fouls']]\n    project_id=os.environ['GCP_PROJECT_NAME']\n    event_data = kwargs.get('event_data')\n    if event_data is None:\n        event_data =  [{\"h_three_points_pct\": 99.8, \"h_three_points_att\": 99.0, \"h_two_points_pct\": 41.0, \"h_two_points_att\": 39.0, \"h_offensive_rebounds\": 5.0, \"h_defensive_rebounds\": 30.0, \"h_turnovers\": 9.0, \"h_steals\": 0.0, \"h_blocks\": 4.0, \"h_personal_fouls\": 15.0, \"h_points\": 70.0, \"a_three_points_pct\": 41.7, \"a_three_points_att\": 12.0, \"a_two_points_pct\": 46.3, \"a_two_points_att\": 54.0, \"a_offensive_rebounds\": 5.0, \"a_defensive_rebounds\": 27.0, \"a_turnovers\": 10.0, \"a_steals\": 2.0, \"a_blocks\": 5.0, \"a_personal_fouls\": 17.0, \"a_points\": 73.0},{\"h_three_points_pct\": 34.8, \"h_three_points_att\": 23.0, \"h_two_points_pct\": 41.0, \"h_two_points_att\": 39.0, \"h_offensive_rebounds\": 5.0, \"h_defensive_rebounds\": 30.0, \"h_turnovers\": 9.0, \"h_steals\": 0.0, \"h_blocks\": 4.0, \"h_personal_fouls\": 15.0, \"h_points\": 70.0, \"a_three_points_pct\": 41.7, \"a_three_points_att\": 12.0, \"a_two_points_pct\": 46.3, \"a_two_points_att\": 54.0, \"a_offensive_rebounds\": 5.0, \"a_defensive_rebounds\": 27.0, \"a_turnovers\": 10.0, \"a_steals\": 2.0, \"a_blocks\": 5.0, \"a_personal_fouls\": 17.0, \"a_points\": 73.0}]\n    if type(event_data)==str:\n        event_data = json.loads(event_data)\n    \n\n    client = MlflowClient(tracking_uri=\"http://mlflow:5000\")\n    model_name = \"basketball_predictor\"\n    latest_versions = client.get_latest_versions(name=model_name)\n\n    for version in latest_versions:\n        logged_model=version.source[:-5]\n    print(logged_model)\n\n    message={\"event\":event_data,\"logged_model\":logged_model,\"project_id\": project_id}\n    publish(message)\n    pred=listen(message)\n    print(pred)\n    return pred\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "custom/starry_elm.py", "language": "python", "type": "custom", "uuid": "starry_elm"}, "data_exporters/send_to_monitoring.py:data_exporter:python:send to monitoring": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nimport requests\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs)->List:\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    r=requests.get('http://localhost:6789/api/pipelines?include_schedules=1')\n    j=r.json()\n    for el in j['pipelines']:\n        for sh in el['schedules']:\n            if sh['name']=='add_to_monitoring' and sh['schedule_type']=='api':\n                trigger_id=sh['id']\n    r=requests.post(url='http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/96a3147e2ac749929eadac81c581d8e3', data=data)\n\n    return data\n\n", "file_path": "data_exporters/send_to_monitoring.py", "language": "python", "type": "data_exporter", "uuid": "send_to_monitoring"}, "data_exporters/retrain.py:data_exporter:python:retrain": {"content": "from mage_ai.orchestration.triggers.api import trigger_pipeline\nif 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nfrom mlflow.entities import ViewType\nimport mlflow\nimport requests\n\n\n@data_exporter\ndef trigger(data, *args, **kwargs):\n    train_year, val_year, use_dv, logged_model = data\n\n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n\n    runs_mae = mlflow.search_runs(\n    experiment_names=[\"Model evaluation with Evidently\"],\n    run_view_type=ViewType.ACTIVE_ONLY,\n    max_results=100,\n    filter_string=f\"params.model = '{logged_model}' and metrics.current_sample>150 and metrics.significant_mae_diff=1\")\n    \n    runs_drift = mlflow.search_runs(\n    experiment_names=[\"Model evaluation with Evidently\"],\n    run_view_type=ViewType.ACTIVE_ONLY,\n    max_results=100,\n    #order_by=[\"metrics.rmse ASC\"],\n    filter_string=f\"params.model = '{logged_model}' and metrics.current_sample>150 and metrics.total_detected>2\")\n\n    if len(runs_drift)+len(runs_mae)>0:\n        if train_year<2017:\n            train_year=train_year+1\n            val_year=train_year+1\n        r=requests.get('http://localhost:6789/api/pipelines?include_schedules=1')\n        j=r.json()\n        for el in j['pipelines']:\n            for sh in el['schedules']:\n                if sh['name']=='GLOBAL_DATA_PRODUCT_TRIGGER' and sh['schedule_type']=='api':\n                    trigger_id=sh['id']\n        r=requests.post(\n            f'http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/fe9b2f2228444b739bf9e191304675d7', data=str({\n  \"pipeline_run\": {\n    \"variables\": {\n      \"use_dv\": 0,\n      \"train_year\": train_year,\n      \"val_year\": val_year\n      }}}).replace(\"'\",'\"'))\n        print(r.json())\n\n\n\n        \n\n\n    \n    \n", "file_path": "data_exporters/retrain.py", "language": "python", "type": "data_exporter", "uuid": "retrain"}, "data_exporters/build.py:data_exporter:python:build": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nfrom sklearn.feature_extraction import DictVectorizer\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    X_train, y_train, X_val, y_val, train_year, val_year, use_dv = data[0], data[1], data[2], data[3], data[4], data[5], data[6]\n    if use_dv==1:\n        dv = DictVectorizer()\n        train_dicts = X_train.to_dict(orient='records')\n        X_train = dv.fit_transform(train_dicts)\n    else:\n        dv = None\n    #train_year = kwargs['train_year'] \n    #val_year = kwargs['val_year']\n\n\n    if X_val is not None and use_dv==1:\n        val_dicts = X_val.to_dict(orient='records')\n        X_val = dv.transform(val_dicts)\n\n    return  X_train, X_val, y_train, y_val, dv, train_year, val_year, use_dv\n    # Specify your data exporting logic here\n\n\n", "file_path": "data_exporters/build.py", "language": "python", "type": "data_exporter", "uuid": "build"}, "data_exporters/trigger_training.py:data_exporter:python:trigger training": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nimport requests\n\n\n@data_exporter\ndef trigger(*args, **kwargs):\n    r=requests.get('http://localhost:6789/api/pipelines?include_schedules=1')\n    j=r.json()\n    for el in j['pipelines']:\n        for sh in el['schedules']:\n            if sh['name']=='trigger_training' and sh['schedule_type']=='api':\n                trigger_id=sh['id']\n    requests.post(f'http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/73dd5fc8d6c84d9aa9d714fa408b2850')", "file_path": "data_exporters/trigger_training.py", "language": "python", "type": "data_exporter", "uuid": "trigger_training"}, "data_loaders/extract_from_bq.py:data_loader:python:extract from bq": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.bigquery import BigQuery\nfrom mage_ai.io.config import ConfigFileLoader\nfrom os import path\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef define_query(year):\n    query = f'''WITH previous_games as \n(SELECT \n  team_id, \n  gametime, \n  game_id,\n  LAG(three_points_pct) OVER team AS three_points_pct,\n  LAG(three_points_att) OVER team AS three_points_att,\n  LAG(two_points_pct) OVER team AS two_points_pct,\n  LAG(two_points_att) OVER team AS two_points_att,\n  LAG(offensive_rebounds) OVER team AS offensive_rebounds,\n  LAG(defensive_rebounds) OVER team AS defensive_rebounds,\n  LAG(turnovers) OVER team AS turnovers,\n  LAG(steals) OVER team AS steals,\n  LAG(blocks) OVER team AS blocks,\n  LAG(personal_fouls) OVER team AS personal_fouls,\n  LAG(points) OVER team AS points,\n  LAG(gametime) OVER team AS gametime_lag,\n  LAG(game_id) OVER team AS game_id_lag\n  FROM bigquery-public-data.ncaa_basketball.mbb_teams_games_sr\n  WHERE EXTRACT(year FROM gametime)={year} AND coverage='full'\n  WINDOW team AS (PARTITION BY team_id ORDER BY gametime ASC))\n\nSELECT\n  games.game_id,\n  pg_h.three_points_pct AS h_three_points_pct,\n  pg_h.three_points_att AS h_three_points_att,\n  pg_h.two_points_pct as h_two_points_pct,\n  pg_h.two_points_att as h_two_points_att,\n  pg_h.offensive_rebounds as h_offensive_rebounds,\n  pg_h.defensive_rebounds as h_defensive_rebounds,\n  pg_h.turnovers as h_turnovers,\n  pg_h.steals as h_steals,\n  pg_h.blocks as h_blocks,\n  pg_h.personal_fouls as h_personal_fouls ,\n  games.h_points as h_points,\n  pg_a.three_points_pct as a_three_points_pct,\n  pg_a.three_points_att as a_three_points_att,\n  pg_a.two_points_pct as a_two_points_pct,\n  pg_a.two_points_att as a_two_points_att,\n  pg_a.offensive_rebounds as a_offensive_rebounds,\n  pg_a.defensive_rebounds as a_defensive_rebounds,\n  pg_a.turnovers as a_turnovers,\n  pg_a.steals as a_steals,\n  pg_a.blocks as a_blocks,\n  pg_a.personal_fouls as a_personal_fouls,\n  games.a_points as a_points\n \nFROM\n  bigquery-public-data.ncaa_basketball.mbb_games_sr games\n  JOIN previous_games pg_h ON games.h_id=pg_h.team_id AND games.game_id=pg_h.game_id\n  JOIN previous_games pg_a ON games.a_id=pg_a.team_id AND games.game_id=pg_a.game_id\n\nWHERE\n  EXTRACT(year\n  FROM games.gametime)={year}\n  AND pg_h.game_id_lag IS NOT NULL\n  AND pg_a.game_id_lag IS NOT NULL'''\n    return query\n@data_loader\ndef load_data_from_big_query(*args, **kwargs):\n    print('start')\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n    train_year = kwargs.get('train_year')\n    if train_year is None:\n        train_year = 2013\n    val_year = kwargs.get('val_year')\n    if val_year is None:\n        val_year = 2014\n    use_dv = kwargs.get('use_dv')\n    if use_dv is None:\n        use_dv = 0\n\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n    query=define_query(train_year)\n    train_data=BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n    query=define_query(val_year)\n    val_data=BigQuery.with_config(ConfigFileLoader(config_path, config_profile)).load(query)\n    return [train_data, val_data, train_year, val_year, use_dv]\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "data_loaders/extract_from_bq.py", "language": "python", "type": "data_loader", "uuid": "extract_from_bq"}, "sensors/detect_new_data.py:sensor:python:detect new data": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.google_cloud_storage import GoogleCloudStorage\nfrom os import path\n\nif 'sensor' not in globals():\n    from mage_ai.data_preparation.decorators import sensor\n\n\n@sensor\ndef check_condition(*args, **kwargs) -> bool:\n    \"\"\"\n    Template code for checking if a file or folder exists in a Google Cloud Storage bucket.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#googlecloudstorage\n    \"\"\"\n\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    bucket_name = 'your_bucket_name'\n    object_key = 'your_object_key'\n\n    return GoogleCloudStorage.with_config(ConfigFileLoader(config_path, config_profile)).exists(\n        bucket_name,\n        object_key,\n    )", "file_path": "sensors/detect_new_data.py", "language": "python", "type": "sensor", "uuid": "detect_new_data"}, "transformers/prepare_data.py:transformer:python:prepare data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\ndef prepare_data(val,coltype):\n    if coltype=='pct':\n        if val>1:\n            val=val/100\n    return val\ndef transform_data(df, *args, **kwargs):\n    \n    df['target'] = df.h_points-df.a_points\n    df=df.fillna(0)\n    for col in 'h_two_points_pct','h_three_points_pct','a_two_points_pct','a_three_points_pct':\n        df[col]=df[col].map(lambda x: prepare_data(x,'pct'))\n        df=df[df[col]<1]\n    \n\n    X=df[['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls']]\n    y=df.target\n    return X,y\n\n@transformer\ndef transform(data, *args, **kwargs):\n    train_data, val_data, train_year, val_year, use_dv = data[0], data[1], data[2], data[3],  data[4]\n    X_train,y_train=transform_data(train_data)\n    X_val,y_val=transform_data(val_data)\n\n    return X_train, y_train, X_val, y_val, train_year, val_year, use_dv\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/prepare_data.py", "language": "python", "type": "transformer", "uuid": "prepare_data"}, "transformers/transform.py:transformer:python:transform": {"content": "from mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nfrom mage_ai.io.bigquery import BigQuery\nfrom os import path\nfrom pandas import DataFrame\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@transformer\ndef transform_in_bigquery(*args, **kwargs) -> DataFrame:\n    \"\"\"\n    Performs a transformation in BigQuery\n    \"\"\"\n    config_path = path.join(get_repo_path(), 'io_config.yaml')\n    config_profile = 'default'\n\n    # Specify your SQL transformation query\n    query = 'your transformation_query'\n\n    # Specify table to sample data from. Use to visualize changes to table.\n    sample_table = 'table_to_sample_data_from'\n    sample_schema = 'schema_of_table_to_sample'\n    sample_size = 10_000\n\n    with BigQuery.with_config(ConfigFileLoader(config_path, config_profile)) as loader:\n        # Write queries to transform your dataset with\n        loader.execute(query)\n        return loader.sample(sample_schema, sample_size, sample_table)\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/transform.py", "language": "python", "type": "transformer", "uuid": "transform"}, "transformers/model_registry.py:transformer:python:model registry": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom mlflow.entities import ViewType\nfrom mlflow import MlflowClient\nimport mlflow\n@transformer\ndef transform(data, *args, **kwargs):\n    print(data)\n    train_year=data[0]\n    print(train_year)\n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n    mlflow.set_experiment(\"basketball\")\n    runs = mlflow.search_runs(\n        experiment_names=[\"basketball\"],\n        run_view_type=ViewType.ACTIVE_ONLY,\n        max_results=1,\n        order_by=[\"metrics.rmse ASC\"],\n        filter_string=f'tags.train_year=\"{train_year}\"')\n    run_id = runs.loc[0,'run_id']\n    print(run_id)\n\n    model_uri = f\"runs:/{run_id}/model\"\n\n    mlflow.register_model(model_uri=model_uri, name=\"basketball_predictor\")\n\n    return 1\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/model_registry.py", "language": "python", "type": "transformer", "uuid": "model_registry"}, "transformers/evidently_report.py:transformer:python:evidently report": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport evidently\nimport mlflow\nimport pandas as pd\nfrom evidently.pipeline.column_mapping import ColumnMapping\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, RegressionPreset\nimport numpy as np\ndef eval_drift(reference, production, column_mapping):\n\n    data_drift_report = Report(metrics=[DataDriftPreset()])\n    data_drift_report.run(reference_data=reference, current_data=production, column_mapping=column_mapping)\n    report = data_drift_report.as_dict()\n    drifts = []\n    for feature in column_mapping.numerical_features:\n        drifts.append((feature, report[\"metrics\"][1][\"result\"][\"drift_by_columns\"][feature][\"drift_score\"]))\n    drifts.append(('total_detected',sum([int(report[\"metrics\"][1][\"result\"][\"drift_by_columns\"][feature][\"drift_detected\"]) for feature in column_mapping.numerical_features])))\n    return drifts\n\ndef eval_regression(reference, production, column_mapping):\n\n    regression_report = Report(metrics=[RegressionPreset()])\n    regression_report.run(reference_data=reference, current_data=production, column_mapping=column_mapping)\n    report = regression_report.as_dict()\n    mae_diff = (report[\"metrics\"][0]['result']['current']['mean_abs_error']-report[\"metrics\"][0]['result']['current']['abs_error_std']*1.96)-(report[\"metrics\"][0]['result']['reference']['mean_abs_error']+report[\"metrics\"][0]['result']['reference']['abs_error_std']*1.96)\n    return mae_diff\n\n@transformer\ndef transform(data, data_2, *args, **kwargs):\n\n    data, logged_model = data[0], data[1]\n    reference, train_year, val_year, use_dv = data_2\n    reference = reference.astype(np.float64)\n    data_columns = ColumnMapping()\n    data_columns.numerical_features = ['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls']\n    data_columns.target = 'target'\n    data_columns.prediction = 'prediction'\n        \n    #log into MLflow\n    mlflow.set_tracking_uri(\"http://mlflow:5000\")\n    mlflow.set_experiment('Model evaluation with Evidently')\n\n    #start new run\n    with mlflow.start_run() as run: \n        \n        mlflow.log_param(\"model\", logged_model)\n\n        # Log metrics\n        metrics = eval_drift(reference[['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls']], \n                            data[['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls']], \n                            column_mapping=data_columns)\n        for feature in metrics:\n            mlflow.log_metric(feature[0], round(feature[1], 3))\n        \n        if 'target' in data.columns:\n            mae_diff = eval_regression(reference, \n                            data, \n                            column_mapping=data_columns)\n            \n            mlflow.log_metric('mae_diff', round(mae_diff, 3))\n            mlflow.log_metric('significant_mae_diff', mae_diff>0)\n            mlflow.log_metric('current_sample', len(data))\n\n\n\n        \n    return train_year, val_year, use_dv, logged_model\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/evidently_report.py", "language": "python", "type": "transformer", "uuid": "evidently_report"}, "transformers/training.py:transformer:python:training": {"content": "from typing import Callable, Dict, Tuple, Union\nimport mlflow\nfrom pandas import Series\nfrom scipy.sparse._csr import csr_matrix\nfrom sklearn.base import BaseEstimator\nfrom utils.sklearn import load_class, tune_hyperparameters\n\nif 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\n\n\n@transformer\ndef hyperparameter_tuning(\n    model_class_name: str,\n    training_set: Dict[str, Union[Series, csr_matrix]],\n    *args,\n    **kwargs,\n) -> Tuple[\n    Dict[str, Union[bool, float, int, str]],\n    csr_matrix,\n    Series,\n    Callable[..., BaseEstimator],\n]:\n    X_train, X_val, y_train, y_val, dv, train_year, val_year, use_dv = training_set['build']\n    print(train_year)\n    model_class = load_class(model_class_name)\n\n    if model_class_name=='linear_model.LinearRegression':\n        max_evaluations=1\n    else:\n        max_evaluations=2\n    max_evaluations=max_evaluations\n    hyperparameters = tune_hyperparameters(\n        model_class=model_class,\n        X_train=X_train,\n        y_train=y_train,\n        X_val=X_val,\n        y_val=y_val,\n        max_evaluations=max_evaluations,\n        random_state=kwargs.get('random_state'),\n        dv=dv,\n        train_year = train_year,\n        val_year = val_year,\n        use_dv = use_dv\n        )\n\n    return train_year, hyperparameters, dict(cls=model_class, name=model_class_name)\n", "file_path": "transformers/training.py", "language": "python", "type": "transformer", "uuid": "training"}, "transformers/models.py:transformer:python:models": {"content": "from typing import Dict, List, Tuple\n\nif 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\n\n\n@custom\ndef models(*args, **kwargs) -> Tuple[List[str], List[Dict[str, str]]]:\n    \"\"\"\n    models: comma separated strings\n        linear_model.Lasso\n        linear_model.LinearRegression\n        svm.LinearSVR\n        ensemble.ExtraTreesRegressor\n        ensemble.GradientBoostingRegressor\n        ensemble.RandomForestRegressor\n    \"\"\"\n    model_names: str = kwargs.get(\n        'models', 'linear_model.LinearRegression,linear_model.Lasso,ensemble.RandomForestRegressor'\n    )\n    child_data: List[str] = [\n        model_name.strip() for model_name in model_names.split(',')\n    ]\n    child_metadata: List[Dict] = [\n        dict(block_uuid=model_name.split('.')[-1]) for model_name in child_data\n    ]\n\n    return child_data, child_metadata\n", "file_path": "transformers/models.py", "language": "python", "type": "transformer", "uuid": "models"}, "transformers/fill_in_missing_values.py:transformer:python:fill in missing values": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport pandas as pd\nimport evidently\n\n@custom\ndef transform_custom(data, *args, **kwargs):\n    #data = kwargs.get('data')\n    data2 = {'features': {'h_three_points_pct': {'0': 0.10800000000000001, '1': 0.348}, 'h_three_points_att': {'0': 99.0, '1': 23.0}, 'h_two_points_pct': {'0': 0.41, '1': 0.41}, 'h_two_points_att': {'0': 39.0, '1': 39.0}, 'h_offensive_rebounds': {'0': 5.0, '1': 5.0}, 'h_defensive_rebounds': {'0': 30.0, '1': 30.0}, 'h_turnovers': {'0': 9.0, '1': 9.0}, 'h_steals': {'0': 0.0, '1': 0.0}, 'h_blocks': {'0': 4.0, '1': 4.0}, 'h_personal_fouls': {'0': 15.0, '1': 15.0}, 'h_points': {'0': 70.0, '1': 70.0}, 'a_three_points_pct': {'0': 0.41700000000000004, '1': 0.41700000000000004}, 'a_three_points_att': {'0': 12.0, '1': 12.0}, 'a_two_points_pct': {'0': 0.46299999999999997, '1': 0.46299999999999997}, 'a_two_points_att': {'0': 54.0, '1': 54.0}, 'a_offensive_rebounds': {'0': 5.0, '1': 5.0}, 'a_defensive_rebounds': {'0': 27.0, '1': 27.0}, 'a_turnovers': {'0': 10.0, '1': 10.0}, 'a_steals': {'0': 2.0, '1': 2.0}, 'a_blocks': {'0': 5.0, '1': 5.0}, 'a_personal_fouls': {'0': 17.0, '1': 17.0}, 'a_points': {'0': 73.0, '1': 73.0}}, 'logged_model': 'gs://ibphilippov-mlops-storage/artifacts/719fb9ce3cf143408d7e63298521794e/artifacts/', 'prediction': [8.340391221695402, 9.876255483351072]}\n    event_data = pd.DataFrame.from_dict(data2['features'])\n    event_data = event_data[['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','h_points','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls','a_points']]\n    if type(data['prediction'])==dict:\n        event_pred = pd.DataFrame([data['prediction']],columns=['prediction'])\n    else:\n        event_pred = pd.DataFrame(data['prediction'],columns=['prediction'])\n    for i in range(len(data['prediction'])):\n        event_data.loc[i,'prediction']=data['prediction'][i]\n    if 'h_points' in event_data.columns and 'a_points' in event_data.columns:\n        event_data['target']=event_data['h_points']-event_data['a_points']\n        event_data = event_data.drop(columns=['h_points','a_points'])\n    logged_model=data['logged_model']\n    return event_data, logged_model\n\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "transformers/fill_in_missing_values.py", "language": "python", "type": "transformer", "uuid": "fill_in_missing_values"}, "transformers/prepare_reference_data.py:transformer:python:prepare reference data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport mlflow\n\n\n@transformer\ndef transform(data, data_2, *args, **kwargs):\n    X_train, X_val, y_train, y_val, dv, train_year, val_year, use_dv = data_2['build']\n    model = mlflow.pyfunc.load_model(data[1])\n    X_train['prediction']= model.predict(X_train)\n    X_train['target']=y_train\n    return X_train, train_year, val_year, use_dv\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "transformers/prepare_reference_data.py", "language": "python", "type": "transformer", "uuid": "prepare_reference_data"}, "pipelines/deployment/__init__.py:pipeline:python:deployment/  init  ": {"content": "", "file_path": "pipelines/deployment/__init__.py", "language": "python", "type": "pipeline", "uuid": "deployment/__init__"}, "pipelines/deployment/interactions.yaml:pipeline:yaml:deployment/interactions": {"content": "blocks: {}\nlayout: []\n", "file_path": "pipelines/deployment/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "deployment/interactions"}, "pipelines/deployment/metadata.yaml:pipeline:yaml:deployment/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - send_to_monitoring\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: make_prediction\n  retry_config: null\n  status: updated\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: make_prediction\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: data_exporters/send_to_monitoring.py\n    file_source:\n      path: data_exporters/send_to_monitoring.py\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: send_to_monitoring\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - make_prediction\n  uuid: send_to_monitoring\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config:\n  pipeline_run_limit: 1\nconditionals: []\ncreated_at: '2024-08-04 15:04:20.334862+00:00'\ndata_integration: null\ndescription: deployment\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: deployment\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: deployment\nvariables_dir: /home/src/mage_data/Basketball_results_prediction\nwidgets: []\n", "file_path": "pipelines/deployment/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "deployment/metadata"}, "pipelines/deployment/triggers.yaml:pipeline:yaml:deployment/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2024-08-06 21:27:27.307003\n  name: basketball_predict\n  pipeline_uuid: deployment\n  schedule_interval: null\n  schedule_type: api\n  settings: null\n  sla: null\n  start_time: 2024-08-06 18:12:00\n  status: active\n  token: d7989ff289e04a32becd553b733fa5fc\n  variables: {}\n", "file_path": "pipelines/deployment/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "deployment/triggers"}, "pipelines/get_data_from_bq/__init__.py:pipeline:python:get data from bq/  init  ": {"content": "", "file_path": "pipelines/get_data_from_bq/__init__.py", "language": "python", "type": "pipeline", "uuid": "get_data_from_bq/__init__"}, "pipelines/get_data_from_bq/metadata.yaml:pipeline:yaml:get data from bq/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - prepare_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: extract_from_bq\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_loader\n  upstream_blocks: []\n  uuid: extract_from_bq\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - build\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: prepare_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - extract_from_bq\n  uuid: prepare_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - trigger_training\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: build\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - prepare_data\n  uuid: build\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: trigger_training\n  retry_config: null\n  status: executed\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - build\n  uuid: trigger_training\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-07-17 14:34:47.350625+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: get_data_from_bq\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: get_data_from_bq\nvariables_dir: /home/src/mage_data/Basketball_results_prediction\nwidgets: []\n", "file_path": "pipelines/get_data_from_bq/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "get_data_from_bq/metadata"}, "pipelines/get_data_from_bq/triggers.yaml:pipeline:yaml:get data from bq/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2024-08-10 18:29:28.856558+00:00\n  name: GLOBAL_DATA_PRODUCT_TRIGGER\n  pipeline_uuid: get_data_from_bq\n  schedule_interval: null\n  schedule_type: api\n  settings: null\n  sla: null\n  start_time: 2024-08-09 21:11:00\n  status: active\n  token: fe9b2f2228444b739bf9e191304675d7\n  variables: {}\n", "file_path": "pipelines/get_data_from_bq/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "get_data_from_bq/triggers"}, "pipelines/train_sklearn/__init__.py:pipeline:python:train sklearn/  init  ": {"content": "", "file_path": "pipelines/train_sklearn/__init__.py", "language": "python", "type": "pipeline", "uuid": "train_sklearn/__init__"}, "pipelines/train_sklearn/interactions.yaml:pipeline:yaml:train sklearn/interactions": {"content": "blocks: {}\nlayout: []\n", "file_path": "pipelines/train_sklearn/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "train_sklearn/interactions"}, "pipelines/train_sklearn/metadata.yaml:pipeline:yaml:train sklearn/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    global_data_product:\n      uuid: gdp_train_data\n  downstream_blocks:\n  - training\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: training_set\n  retry_config: null\n  status: executed\n  timeout: null\n  type: global_data_product\n  upstream_blocks: []\n  uuid: training_set\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    dynamic: true\n  downstream_blocks:\n  - training\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: set_models\n  retry_config: null\n  status: executed\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: set_models\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    reduce_output: true\n  downstream_blocks:\n  - model_registry\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: training\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - set_models\n  - training_set\n  uuid: training\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: model_registry\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - training\n  uuid: model_registry\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-07-27 10:13:58.925393+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: null\nextensions: {}\nname: train sklearn\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: train_sklearn\nvariables_dir: /home/src/mage_data/Basketball_results_prediction\nwidgets: []\n", "file_path": "pipelines/train_sklearn/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "train_sklearn/metadata"}, "pipelines/train_sklearn/triggers.yaml:pipeline:yaml:train sklearn/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2024-08-10 09:05:26.867680\n  name: trigger_training\n  pipeline_uuid: train_sklearn\n  schedule_interval: null\n  schedule_type: api\n  settings: null\n  sla: null\n  start_time: 2024-08-10 09:05:00\n  status: active\n  token: 73dd5fc8d6c84d9aa9d714fa408b2850\n  variables: {}\n", "file_path": "pipelines/train_sklearn/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "train_sklearn/triggers"}, "pipelines/monitoring/__init__.py:pipeline:python:monitoring/  init  ": {"content": "", "file_path": "pipelines/monitoring/__init__.py", "language": "python", "type": "pipeline", "uuid": "monitoring/__init__"}, "pipelines/monitoring/interactions.yaml:pipeline:yaml:monitoring/interactions": {"content": "blocks: {}\nlayout: []\n", "file_path": "pipelines/monitoring/interactions.yaml", "language": "yaml", "type": "pipeline", "uuid": "monitoring/interactions"}, "pipelines/monitoring/metadata.yaml:pipeline:yaml:monitoring/metadata": {"content": "blocks:\n- all_upstream_blocks_executed: true\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - prepare_reference_data\n  - evidently_report\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: prepare_current_data\n  retry_config: null\n  status: updated\n  timeout: null\n  type: custom\n  upstream_blocks: []\n  uuid: prepare_current_data\n- all_upstream_blocks_executed: true\n  color: null\n  configuration:\n    global_data_product:\n      uuid: gdp_train_data\n  downstream_blocks:\n  - prepare_reference_data\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: reference_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: global_data_product\n  upstream_blocks: []\n  uuid: reference_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks:\n  - evidently_report\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: prepare_reference_data\n  retry_config: null\n  status: executed\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - prepare_current_data\n  - reference_data\n  uuid: prepare_reference_data\n- all_upstream_blocks_executed: false\n  color: null\n  configuration:\n    file_path: transformers/evidently_report.py\n    file_source:\n      path: transformers/evidently_report.py\n  downstream_blocks:\n  - retrain\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: evidently_report\n  retry_config: null\n  status: updated\n  timeout: null\n  type: transformer\n  upstream_blocks:\n  - prepare_current_data\n  - prepare_reference_data\n  uuid: evidently_report\n- all_upstream_blocks_executed: false\n  color: null\n  configuration: {}\n  downstream_blocks: []\n  executor_config: null\n  executor_type: local_python\n  has_callback: false\n  language: python\n  name: retrain\n  retry_config: null\n  status: updated\n  timeout: null\n  type: data_exporter\n  upstream_blocks:\n  - evidently_report\n  uuid: retrain\ncache_block_output_in_memory: false\ncallbacks: []\nconcurrency_config: {}\nconditionals: []\ncreated_at: '2024-08-09 13:45:36.146119+00:00'\ndata_integration: null\ndescription: null\nexecutor_config: {}\nexecutor_count: 1\nexecutor_type: local_python\nextensions: {}\nname: monitoring\nnotification_config: {}\nremote_variables_dir: null\nretry_config: {}\nrun_pipeline_in_one_process: false\nsettings:\n  triggers: null\nspark_config: {}\ntags: []\ntype: python\nuuid: monitoring\nvariables_dir: /home/src/mage_data/Basketball_results_prediction\nwidgets: []\n", "file_path": "pipelines/monitoring/metadata.yaml", "language": "yaml", "type": "pipeline", "uuid": "monitoring/metadata"}, "pipelines/monitoring/triggers.yaml:pipeline:yaml:monitoring/triggers": {"content": "triggers:\n- description: null\n  envs: []\n  last_enabled_at: 2024-08-09 21:09:38.082657\n  name: add_to_monitoring\n  pipeline_uuid: monitoring\n  schedule_interval: null\n  schedule_type: api\n  settings: null\n  sla: null\n  start_time: 2024-08-09 14:11:00\n  status: active\n  token: 96a3147e2ac749929eadac81c581d8e3\n  variables: {}\n", "file_path": "pipelines/monitoring/triggers.yaml", "language": "yaml", "type": "pipeline", "uuid": "monitoring/triggers"}, "/home/src/Basketball_results_prediction/custom/make_prediction.py:custom:python:home/src/Basketball results prediction/custom/make prediction": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\nfrom mage_ai.settings.repo import get_repo_path\nfrom mage_ai.io.config import ConfigFileLoader\nimport json\nif 'data_loader' not in globals():\n    from mage_ai.data_preparation.decorators import data_loader\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nfrom concurrent import futures\nfrom google.cloud import pubsub_v1\nfrom typing import Callable\nimport os\nfrom os import path\nfrom concurrent.futures import TimeoutError\nfrom mlflow import MlflowClient\nfrom google.oauth2 import service_account\n\n\n#os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'credentials.json'\n#os.environ['GCP_PROJECT_NAME'] = 'ibphilippov-mlops' #!!!! \u0443\u0434\u0430\u043b\u0438\u0442\u044c\n#os.environ['ARTIFACT_STORAGE'] = 'ibphilippov-mlops-storage'\n\ndef get_callback(\n    publish_future: pubsub_v1.publisher.futures.Future, data: str\n) -> Callable[[pubsub_v1.publisher.futures.Future], None]:\n    def callback(publish_future: pubsub_v1.publisher.futures.Future) -> None:\n        try:\n            # Wait 60 seconds for the publish call to succeed.\n            print(publish_future.result(timeout=60))\n        except futures.TimeoutError:\n            print(f\"Publishing {data} timed out.\")\n\n    return callback\n\ndef callback_listen(message: pubsub_v1.subscriber.message.Message) -> None:\n    global processed_data\n    print(f\"Received {message}.\")\n    processed_data.append(json.loads(message.data.decode('utf-8')))\n    message.ack()\n    return processed_data\n\ndef listen(message_sent):\n    global processed_data\n    project_id=os.environ['GCP_PROJECT_NAME']\n    timeout = 15\n\n    subscriber = pubsub_v1.SubscriberClient(credentials=service_account.Credentials.from_service_account_file(\n        'credentials.json'))\n    subscription_path = subscriber.subscription_path(project_id, 'read')\n    processed_data=[]\n    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback_listen)\n    print(f\"Listening for messages on {subscription_path}..\\n\")\n\n    # Wrap subscriber in a 'with' block to automatically call close() when done.\n    with subscriber:\n        try:\n            streaming_pull_future.result(timeout=timeout)\n        except TimeoutError:\n            streaming_pull_future.cancel()  # Trigger the shutdown.\n            streaming_pull_future.result() \n    for message in processed_data:\n        if 1==1:#message['data']==message_sent['event']:\n            return message['prediction'], message['prepared_X']\n\ndef publish(data):\n    project_id=os.environ['GCP_PROJECT_NAME']\n    topic_id = \"predictions-input\"\n    publisher = pubsub_v1.PublisherClient(credentials=service_account.Credentials.from_service_account_file(\n        'credentials.json'))\n    topic_path = publisher.topic_path(project_id, topic_id)\n    publish_futures = []\n    data = json.dumps(data).encode('utf-8')\n    publish_future = publisher.publish(topic_path, data)\n    # Non-blocking. Publish failures are handled in the callback function.\n    publish_future.add_done_callback(get_callback(publish_future, data))\n    publish_futures.append(publish_future)\n\n    # Wait for all the publish futures to resolve before exiting.\n    futures.wait(publish_futures, return_when=futures.ALL_COMPLETED)\n\n    print(f\"Published messages with error handler to {topic_path}.\")\n\n\n@custom\ndef get_prediction(*args, **kwargs):\n    \"\"\"\n    Template for loading data from a BigQuery warehouse.\n    Specify your configuration settings in 'io_config.yaml'.\n\n    Docs: https://docs.mage.ai/design/data-loading#bigquery\n    \"\"\"\n\n    project_id=os.environ['GCP_PROJECT_NAME']\n    bucket_name=os.environ['ARTIFACT_STORAGE']\n\n    event_data = kwargs.get('event_data')\n    if event_data is None:\n        event_data =  [{\"h_three_points_pct\": 10.8, \"h_three_points_att\": 99.0, \"h_two_points_pct\": 41.0, \"h_two_points_att\": 39.0, \"h_offensive_rebounds\": 5.0, \"h_defensive_rebounds\": 30.0, \"h_turnovers\": 9.0, \"h_steals\": 0.0, \"h_blocks\": 4.0, \"h_personal_fouls\": 15.0, \"h_points\": 70.0, \"a_three_points_pct\": 41.7, \"a_three_points_att\": 12.0, \"a_two_points_pct\": 46.3, \"a_two_points_att\": 54.0, \"a_offensive_rebounds\": 5.0, \"a_defensive_rebounds\": 27.0, \"a_turnovers\": 10.0, \"a_steals\": 2.0, \"a_blocks\": 5.0, \"a_personal_fouls\": 17.0, \"a_points\": 73.0},{\"h_three_points_pct\": 34.8, \"h_three_points_att\": 23.0, \"h_two_points_pct\": 41.0, \"h_two_points_att\": 39.0, \"h_offensive_rebounds\": 5.0, \"h_defensive_rebounds\": 30.0, \"h_turnovers\": 9.0, \"h_steals\": 0.0, \"h_blocks\": 4.0, \"h_personal_fouls\": 15.0, \"h_points\": 70.0, \"a_three_points_pct\": 41.7, \"a_three_points_att\": 12.0, \"a_two_points_pct\": 46.3, \"a_two_points_att\": 54.0, \"a_offensive_rebounds\": 5.0, \"a_defensive_rebounds\": 27.0, \"a_turnovers\": 10.0, \"a_steals\": 2.0, \"a_blocks\": 5.0, \"a_personal_fouls\": 17.0, \"a_points\": 73.0}]\n    if type(event_data)==str:\n        event_data = json.loads(event_data)\n    \n\n    client = MlflowClient(tracking_uri=\"http://mlflow:5000\")\n    model_name = \"basketball_predictor\"\n    latest_versions = client.get_latest_versions(name=model_name)\n\n    for version in latest_versions:\n        logged_model=version.source[:-5]\n    print(logged_model)\n\n    message={\"event\":event_data,\"logged_model\":logged_model,\"project_id\": project_id,\"bucket_name\": bucket_name}\n    publish(message)\n    prediction,features=listen(message)\n    print(prediction)\n    print({\"features\":features, \"logged_model\":logged_model, \"prediction\": prediction })\n    return {\"features\":features, \"logged_model\":logged_model, \"prediction\": prediction }\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/Basketball_results_prediction/custom/make_prediction.py", "language": "python", "type": "custom", "uuid": "make_prediction"}, "/home/src/Basketball_results_prediction/data_exporters/send_to_monitoring.py:data_exporter:python:home/src/Basketball results prediction/data exporters/send to monitoring": {"content": "if 'data_exporter' not in globals():\n    from mage_ai.data_preparation.decorators import data_exporter\nimport requests\n\n\n@data_exporter\ndef export_data(data, *args, **kwargs):\n    \"\"\"\n    Exports data to some source.\n\n    Args:\n        data: The output from the upstream parent block\n        args: The output from any additional upstream blocks (if applicable)\n\n    Output (optional):\n        Optionally return any object and it'll be logged and\n        displayed when inspecting the block run.\n    \"\"\"\n    r=requests.get('http://localhost:6789/api/pipelines?include_schedules=1')\n    j=r.json()\n    for el in j['pipelines']:\n        for sh in el['schedules']:\n            if sh['name']=='add_to_monitoring' and sh['schedule_type']=='api':\n                trigger_id=sh['id']\n    r=requests.post(url=f'http://localhost:6789/api/pipeline_schedules/{trigger_id}/pipeline_runs/96a3147e2ac749929eadac81c581d8e3', data=str(data).replace(\"'\",'\"'))\n    print(r.json())\n    return data\n\n", "file_path": "/home/src/Basketball_results_prediction/data_exporters/send_to_monitoring.py", "language": "python", "type": "data_exporter", "uuid": "send_to_monitoring"}, "/home/src/Basketball_results_prediction/custom/prepare_current_data.py:custom:python:home/src/Basketball results prediction/custom/prepare current data": {"content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport pandas as pd\nimport evidently\n\n@custom\ndef transform_custom(*args, **kwargs):\n    data = kwargs.get('data')\n    #data = {'features': [{'h_three_points_pct': 0.10800000000000001, 'h_three_points_att': 99.0, 'h_two_points_pct': 0.41, 'h_two_points_att': 39.0, 'h_offensive_rebounds': 5.0, 'h_defensive_rebounds': 30.0, 'h_turnovers': 9.0, 'h_steals': 0.0, 'h_blocks': 4.0, 'h_personal_fouls': 15.0, 'h_points': 70.0, 'a_three_points_pct': 0.41700000000000004, 'a_three_points_att': 12.0, 'a_two_points_pct': 0.46299999999999997, 'a_two_points_att': 54.0, 'a_offensive_rebounds': 5.0, 'a_defensive_rebounds': 27.0, 'a_turnovers': 10.0, 'a_steals': 2.0, 'a_blocks': 5.0, 'a_personal_fouls': 17.0, 'a_points': 73.0}, {'h_three_points_pct': 0.348, 'h_three_points_att': 23.0, 'h_two_points_pct': 0.41, 'h_two_points_att': 39.0, 'h_offensive_rebounds': 5.0, 'h_defensive_rebounds': 30.0, 'h_turnovers': 9.0, 'h_steals': 0.0, 'h_blocks': 4.0, 'h_personal_fouls': 15.0, 'h_points': 70.0, 'a_three_points_pct': 0.41700000000000004, 'a_three_points_att': 12.0, 'a_two_points_pct': 0.46299999999999997, 'a_two_points_att': 54.0, 'a_offensive_rebounds': 5.0, 'a_defensive_rebounds': 27.0, 'a_turnovers': 10.0, 'a_steals': 2.0, 'a_blocks': 5.0, 'a_personal_fouls': 17.0, 'a_points': 73.0}], 'logged_model': 'gs://ibphilippov-mlops-storage/artifacts/719fb9ce3cf143408d7e63298521794e/artifacts/', 'prediction': [8.340391221695402, 9.876255483351072]}\n    data={'features': [{'h_three_points_pct': 0.10800000000000001, 'h_three_points_att': 99.0, 'h_two_points_pct': 0.41, 'h_two_points_att': 39.0, 'h_offensive_rebounds': 5.0, 'h_defensive_rebounds': 30.0, 'h_turnovers': 9.0, 'h_steals': 0.0, 'h_blocks': 4.0, 'h_personal_fouls': 15.0, 'h_points': 70.0, 'a_three_points_pct': 0.41700000000000004, 'a_three_points_att': 12.0, 'a_two_points_pct': 0.46299999999999997, 'a_two_points_att': 54.0, 'a_offensive_rebounds': 5.0, 'a_defensive_rebounds': 27.0, 'a_turnovers': 10.0, 'a_steals': 2.0, 'a_blocks': 5.0, 'a_personal_fouls': 17.0, 'a_points': 73.0}, {'h_three_points_pct': 0.348, 'h_three_points_att': 23.0, 'h_two_points_pct': 0.41, 'h_two_points_att': 39.0, 'h_offensive_rebounds': 5.0, 'h_defensive_rebounds': 30.0, 'h_turnovers': 9.0, 'h_steals': 0.0, 'h_blocks': 4.0, 'h_personal_fouls': 15.0, 'h_points': 70.0, 'a_three_points_pct': 0.41700000000000004, 'a_three_points_att': 12.0, 'a_two_points_pct': 0.46299999999999997, 'a_two_points_att': 54.0, 'a_offensive_rebounds': 5.0, 'a_defensive_rebounds': 27.0, 'a_turnovers': 10.0, 'a_steals': 2.0, 'a_blocks': 5.0, 'a_personal_fouls': 17.0, 'a_points': 73.0}], 'logged_model': 'gs://test-mlops-philippov/1/53d01f5426f947018e1df9fc533d6f31/artifacts/', 'prediction': [-3.592974506799493, 5.767526498804508]}\n    event_data = pd.DataFrame.from_records(data['features'])\n    event_data = event_data[['h_two_points_pct','h_two_points_att','h_three_points_pct','h_three_points_att','h_steals','h_blocks','h_personal_fouls','h_points','a_two_points_pct','a_two_points_att','a_three_points_pct','a_three_points_att','a_steals','a_blocks','a_personal_fouls','a_points']]\n    if type(data['prediction'])==dict:\n        event_pred = pd.DataFrame([data['prediction']],columns=['prediction'])\n    else:\n        event_pred = pd.DataFrame(data['prediction'],columns=['prediction'])\n    event_data=event_data.join(event_pred)\n    if 'h_points' in event_data.columns and 'a_points' in event_data.columns:\n        event_data['target']=event_data['h_points']-event_data['a_points']\n        event_data = event_data.drop(columns=['h_points','a_points'])\n    logged_model=data['logged_model']\n    return event_data, logged_model\n\n\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n", "file_path": "/home/src/Basketball_results_prediction/custom/prepare_current_data.py", "language": "python", "type": "custom", "uuid": "prepare_current_data"}, "/home/src/Basketball_results_prediction/transformers/prepare_reference_data.py:transformer:python:home/src/Basketball results prediction/transformers/prepare reference data": {"content": "if 'transformer' not in globals():\n    from mage_ai.data_preparation.decorators import transformer\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\nimport mlflow\nfrom mlflow import MlflowClient\n\n\n\n@transformer\ndef transform(data, data_2, *args, **kwargs):\n    X_train, X_val, y_train, y_val, dv, train_year, val_year, use_dv = data_2['build']\n    model = mlflow.pyfunc.load_model(data[1])\n    X_train['prediction']= model.predict(X_train)\n    X_train['target']=y_train\n    return X_train, train_year, val_year, use_dv\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'", "file_path": "/home/src/Basketball_results_prediction/transformers/prepare_reference_data.py", "language": "python", "type": "transformer", "uuid": "prepare_reference_data"}}, "custom_block_template": {"custom_templates/blocks/predict:custom:python": {"block_type": "custom", "color": null, "configuration": null, "description": null, "language": "python", "name": null, "pipeline": {}, "tags": [], "user": {}, "template_uuid": "predict", "uuid": "custom_templates/blocks/predict", "content": "if 'custom' not in globals():\n    from mage_ai.data_preparation.decorators import custom\nif 'test' not in globals():\n    from mage_ai.data_preparation.decorators import test\n\n\n@custom\ndef transform_custom(*args, **kwargs):\n    \"\"\"\n    args: The output from any upstream parent blocks (if applicable)\n\n    Returns:\n        Anything (e.g. data frame, dictionary, array, int, str, etc.)\n    \"\"\"\n    # Specify your custom logic here\n\n    return {}\n\n\n@test\ndef test_output(output, *args) -> None:\n    \"\"\"\n    Template code for testing the output of the block.\n    \"\"\"\n    assert output is not None, 'The output is undefined'\n"}}, "mage_template": {"data_loaders/deltalake/s3.py:data_loader:python:Amazon S3:Load a Delta Table from Amazon S3.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_loaders/deltalake/s3.py"}, "data_loaders/deltalake/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Load a Delta Table from Azure Blob Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/deltalake/azure_blob_storage.py"}, "data_loaders/deltalake/gcs.py:data_loader:python:Google Cloud Storage:Load a Delta Table from Google Cloud Storage.:Delta Lake": {"block_type": "data_loader", "description": "Load a Delta Table from Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/deltalake/gcs.py"}, "data_loaders/mongodb.py:data_loader:python:MongoDB:Load data from MongoDB.:Databases (NoSQL)": {"block_type": "data_loader", "description": "Load data from MongoDB.", "groups": ["Databases (NoSQL)"], "language": "python", "name": "MongoDB", "path": "data_loaders/mongodb.py"}, "data_loaders/mssql.py:data_loader:python:MSSQL:Load data from MSSQL.:Databases": {"block_type": "data_loader", "description": "Load data from MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_loaders/mssql.py"}, "data_exporters/deltalake/s3.py:data_exporter:python:Amazon S3:Export data to a Delta Table in Amazon S3.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Amazon S3.", "groups": ["Delta Lake"], "language": "python", "name": "Amazon S3", "path": "data_exporters/deltalake/s3.py"}, "data_exporters/deltalake/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Export data to a Delta Table in Azure Blob Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Azure Blob Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/deltalake/azure_blob_storage.py"}, "data_exporters/deltalake/gcs.py:data_exporter:python:Google Cloud Storage:Export data to a Delta Table in Google Cloud Storage.:Delta Lake": {"block_type": "data_exporter", "description": "Export data to a Delta Table in Google Cloud Storage.", "groups": ["Delta Lake"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/deltalake/gcs.py"}, "data_exporters/mongodb.py:data_exporter:python:MongoDB:Export data to MongoDB.": {"block_type": "data_exporter", "description": "Export data to MongoDB.", "language": "python", "name": "MongoDB", "path": "data_exporters/mongodb.py"}, "data_exporters/mssql.py:data_exporter:python:MSSQL:Export data to MSSQL.:Databases": {"block_type": "data_exporter", "description": "Export data to MSSQL.", "groups": ["Databases"], "language": "python", "name": "MSSQL", "path": "data_exporters/mssql.py"}, "data_loaders/orchestration/triggers/default.jinja:data_loader:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_loader", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_loaders/orchestration/triggers/default.jinja"}, "data_exporters/orchestration/triggers/default.jinja:data_exporter:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "data_exporter", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "data_exporters/orchestration/triggers/default.jinja"}, "callbacks/base.jinja:callback:python:Base template:Base template with empty functions.": {"block_type": "callback", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "callbacks/base.jinja"}, "callbacks/orchestration/triggers/default.jinja:callback:python:Trigger pipeline:Trigger another pipeline to run.:Orchestration": {"block_type": "callback", "description": "Trigger another pipeline to run.", "groups": ["Orchestration"], "language": "python", "name": "Trigger pipeline", "path": "callbacks/orchestration/triggers/default.jinja"}, "conditionals/base.jinja:conditional:python:Base template:Base template with empty functions.": {"block_type": "conditional", "description": "Base template with empty functions.", "language": "python", "name": "Base template", "path": "conditionals/base.jinja"}, "data_loaders/default.jinja:data_loader:python:Base template (generic)": {"block_type": "data_loader", "language": "python", "name": "Base template (generic)", "path": "data_loaders/default.jinja"}, "data_loaders/s3.py:data_loader:python:Amazon S3:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_loaders/s3.py"}, "data_loaders/azure_blob_storage.py:data_loader:python:Azure Blob Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_loaders/azure_blob_storage.py"}, "data_loaders/google_cloud_storage.py:data_loader:python:Google Cloud Storage:Data lakes": {"block_type": "data_loader", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_loaders/google_cloud_storage.py"}, "data_loaders/redshift.py:data_loader:python:Amazon Redshift:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_loaders/redshift.py"}, "data_loaders/bigquery.py:data_loader:python:Google BigQuery:Load data from Google BigQuery.:Data warehouses": {"block_type": "data_loader", "description": "Load data from Google BigQuery.", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_loaders/bigquery.py"}, "data_loaders/snowflake.py:data_loader:python:Snowflake:Data warehouses": {"block_type": "data_loader", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_loaders/snowflake.py"}, "data_loaders/algolia.py:data_loader:python:Algolia:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_loaders/algolia.py"}, "data_loaders/chroma.py:data_loader:python:Chroma:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_loaders/chroma.py"}, "data_loaders/duckdb.py:data_loader:python:DuckDB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_loaders/duckdb.py"}, "data_loaders/mysql.py:data_loader:python:MySQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_loaders/mysql.py"}, "data_loaders/oracledb.py:data_loader:python:Oracle DB:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Oracle DB", "path": "data_loaders/oracledb.py"}, "data_loaders/postgres.py:data_loader:python:PostgreSQL:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_loaders/postgres.py"}, "data_loaders/qdrant.py:data_loader:python:Qdrant:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_loaders/qdrant.py"}, "data_loaders/weaviate.py:data_loader:python:Weaviate:Databases": {"block_type": "data_loader", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_loaders/weaviate.py"}, "data_loaders/api.py:data_loader:python:API:Fetch data from an API request.": {"block_type": "data_loader", "description": "Fetch data from an API request.", "language": "python", "name": "API", "path": "data_loaders/api.py"}, "data_loaders/file.py:data_loader:python:Local file:Load data from a file on your machine.": {"block_type": "data_loader", "description": "Load data from a file on your machine.", "language": "python", "name": "Local file", "path": "data_loaders/file.py"}, "data_loaders/google_sheets.py:data_loader:python:Google Sheets:Load data from a worksheet in Google Sheets.": {"block_type": "data_loader", "description": "Load data from a worksheet in Google Sheets.", "language": "python", "name": "Google Sheets", "path": "data_loaders/google_sheets.py"}, "data_loaders/druid.py:data_loader:python:Druid": {"block_type": "data_loader", "language": "python", "name": "Druid", "path": "data_loaders/druid.py"}, "transformers/default.jinja:transformer:python:Base template (generic)": {"block_type": "transformer", "language": "python", "name": "Base template (generic)", "path": "transformers/default.jinja"}, "transformers/data_warehouse_transformer.jinja:transformer:python:Amazon Redshift:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "redshift", "data_source_handler": "Redshift"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Google BigQuery:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "", "data_source": "bigquery", "data_source_handler": "BigQuery"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:Snowflake:Data warehouses": {"block_type": "transformer", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "snowflake", "data_source_handler": "Snowflake"}}, "transformers/data_warehouse_transformer.jinja:transformer:python:PostgreSQL:Databases": {"block_type": "transformer", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "transformers/data_warehouse_transformer.jinja", "template_variables": {"additional_args": "\n        loader.commit() # Permanently apply database changes", "data_source": "postgres", "data_source_handler": "Postgres"}}, "transformers/transformer_actions/row/drop_duplicate.py:transformer:python:Drop duplicate rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Drop duplicate rows", "path": "transformers/transformer_actions/row/drop_duplicate.py"}, "transformers/transformer_actions/row/filter.py:transformer:python:Filter rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Filter rows", "path": "transformers/transformer_actions/row/filter.py"}, "transformers/transformer_actions/row/remove.py:transformer:python:Remove rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Remove rows", "path": "transformers/transformer_actions/row/remove.py"}, "transformers/transformer_actions/row/sort.py:transformer:python:Sort rows:Row actions": {"block_type": "transformer", "groups": ["Row actions"], "language": "python", "name": "Sort rows", "path": "transformers/transformer_actions/row/sort.py"}, "transformers/transformer_actions/column/average.py:transformer:python:Average value of column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Average value of column", "path": "transformers/transformer_actions/column/average.py"}, "transformers/transformer_actions/column/count_distinct.py:transformer:python:Count unique values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Count unique values in column", "path": "transformers/transformer_actions/column/count_distinct.py"}, "transformers/transformer_actions/column/first.py:transformer:python:First value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "First value in column", "path": "transformers/transformer_actions/column/first.py"}, "transformers/transformer_actions/column/last.py:transformer:python:Last value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Last value in column", "path": "transformers/transformer_actions/column/last.py"}, "transformers/transformer_actions/column/max.py:transformer:python:Maximum value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Maximum value in column", "path": "transformers/transformer_actions/column/max.py"}, "transformers/transformer_actions/column/median.py:transformer:python:Median value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Median value in column", "path": "transformers/transformer_actions/column/median.py"}, "transformers/transformer_actions/column/min.py:transformer:python:Min value in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Min value in column", "path": "transformers/transformer_actions/column/min.py"}, "transformers/transformer_actions/column/sum.py:transformer:python:Sum of all values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Sum of all values in column", "path": "transformers/transformer_actions/column/sum.py"}, "transformers/transformer_actions/column/count.py:transformer:python:Total count of values in column:Column actions:Aggregate": {"block_type": "transformer", "groups": ["Column actions", "Aggregate"], "language": "python", "name": "Total count of values in column", "path": "transformers/transformer_actions/column/count.py"}, "transformers/transformer_actions/column/clean_column_name.py:transformer:python:Clean column name:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Clean column name", "path": "transformers/transformer_actions/column/clean_column_name.py"}, "transformers/transformer_actions/column/fix_syntax_errors.py:transformer:python:Fix syntax errors:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Fix syntax errors", "path": "transformers/transformer_actions/column/fix_syntax_errors.py"}, "transformers/transformer_actions/column/reformat.py:transformer:python:Reformat values in column:Column actions:Formatting": {"block_type": "transformer", "groups": ["Column actions", "Formatting"], "language": "python", "name": "Reformat values in column", "path": "transformers/transformer_actions/column/reformat.py"}, "transformers/transformer_actions/column/select.py:transformer:python:Keep column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Keep column(s)", "path": "transformers/transformer_actions/column/select.py"}, "transformers/transformer_actions/column/remove.py:transformer:python:Remove column(s):Column actions:Column removal": {"block_type": "transformer", "groups": ["Column actions", "Column removal"], "language": "python", "name": "Remove column(s)", "path": "transformers/transformer_actions/column/remove.py"}, "transformers/transformer_actions/column/shift_down.py:transformer:python:Shift row values down:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values down", "path": "transformers/transformer_actions/column/shift_down.py"}, "transformers/transformer_actions/column/shift_up.py:transformer:python:Shift row values up:Column actions:Shift": {"block_type": "transformer", "groups": ["Column actions", "Shift"], "language": "python", "name": "Shift row values up", "path": "transformers/transformer_actions/column/shift_up.py"}, "transformers/transformer_actions/column/normalize.py:transformer:python:Normalize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Normalize data", "path": "transformers/transformer_actions/column/normalize.py"}, "transformers/transformer_actions/column/standardize.py:transformer:python:Standardize data:Column actions:Feature scaling": {"block_type": "transformer", "groups": ["Column actions", "Feature scaling"], "language": "python", "name": "Standardize data", "path": "transformers/transformer_actions/column/standardize.py"}, "transformers/transformer_actions/column/impute.py:transformer:python:Fill in missing values:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Fill in missing values", "path": "transformers/transformer_actions/column/impute.py"}, "transformers/transformer_actions/column/remove_outliers.py:transformer:python:Remove outliers:Column actions:Data cleaning": {"block_type": "transformer", "groups": ["Column actions", "Data cleaning"], "language": "python", "name": "Remove outliers", "path": "transformers/transformer_actions/column/remove_outliers.py"}, "transformers/transformer_actions/column/diff.py:transformer:python:Calculate difference between values:Column actions:Feature extraction": {"block_type": "transformer", "groups": ["Column actions", "Feature extraction"], "language": "python", "name": "Calculate difference between values", "path": "transformers/transformer_actions/column/diff.py"}, "data_exporters/default.jinja:data_exporter:python:Base template (generic)": {"block_type": "data_exporter", "language": "python", "name": "Base template (generic)", "path": "data_exporters/default.jinja"}, "data_exporters/file.py:data_exporter:python:Local file": {"block_type": "data_exporter", "language": "python", "name": "Local file", "path": "data_exporters/file.py"}, "data_exporters/google_sheets.py:data_exporter:python:Google Sheets": {"block_type": "data_exporter", "language": "python", "name": "Google Sheets", "path": "data_exporters/google_sheets.py"}, "data_exporters/s3.py:data_exporter:python:Amazon S3:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "data_exporters/s3.py"}, "data_exporters/azure_blob_storage.py:data_exporter:python:Azure Blob Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Azure Blob Storage", "path": "data_exporters/azure_blob_storage.py"}, "data_exporters/google_cloud_storage.py:data_exporter:python:Google Cloud Storage:Data lakes": {"block_type": "data_exporter", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "data_exporters/google_cloud_storage.py"}, "data_exporters/redshift.py:data_exporter:python:Amazon Redshift:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "data_exporters/redshift.py"}, "data_exporters/bigquery.py:data_exporter:python:Google BigQuery:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "data_exporters/bigquery.py"}, "data_exporters/snowflake.py:data_exporter:python:Snowflake:Data warehouses": {"block_type": "data_exporter", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "data_exporters/snowflake.py"}, "data_exporters/algolia.py:data_exporter:python:Algolia:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Algolia", "path": "data_exporters/algolia.py"}, "data_exporters/chroma.py:data_exporter:python:Chroma:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Chroma", "path": "data_exporters/chroma.py"}, "data_exporters/duckdb.py:data_exporter:python:DuckDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "DuckDB", "path": "data_exporters/duckdb.py"}, "data_exporters/mysql.py:data_exporter:python:MySQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "data_exporters/mysql.py"}, "data_exporters/oracledb.py:data_exporter:python:OracleDB:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "OracleDB", "path": "data_exporters/oracledb.py"}, "data_exporters/postgres.py:data_exporter:python:PostgreSQL:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "data_exporters/postgres.py"}, "data_exporters/qdrant.py:data_exporter:python:Qdrant:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Qdrant", "path": "data_exporters/qdrant.py"}, "data_exporters/weaviate.py:data_exporter:python:Weaviate:Databases": {"block_type": "data_exporter", "groups": ["Databases"], "language": "python", "name": "Weaviate", "path": "data_exporters/weaviate.py"}, "sensors/default.py:sensor:python:Base template (generic)": {"block_type": "sensor", "language": "python", "name": "Base template (generic)", "path": "sensors/default.py"}, "sensors/s3.py:sensor:python:Amazon S3:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Amazon S3", "path": "sensors/s3.py"}, "sensors/google_cloud_storage.py:sensor:python:Google Cloud Storage:Data lakes": {"block_type": "sensor", "groups": ["Data lakes"], "language": "python", "name": "Google Cloud Storage", "path": "sensors/google_cloud_storage.py"}, "sensors/redshift.py:sensor:python:Amazon Redshift:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Amazon Redshift", "path": "sensors/redshift.py"}, "sensors/bigquery.py:sensor:python:Google BigQuery:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Google BigQuery", "path": "sensors/bigquery.py"}, "sensors/snowflake.py:sensor:python:Snowflake:Data warehouses": {"block_type": "sensor", "groups": ["Data warehouses"], "language": "python", "name": "Snowflake", "path": "sensors/snowflake.py"}, "sensors/mysql.py:sensor:python:MySQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "MySQL", "path": "sensors/mysql.py"}, "sensors/postgres.py:sensor:python:PostgreSQL:Databases": {"block_type": "sensor", "groups": ["Databases"], "language": "python", "name": "PostgreSQL", "path": "sensors/postgres.py"}, "data_integrations/sources/base:data_loader:sources:Amazon S3:Data integration data loader block for Amazon S3 sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Amazon S3 sources.", "language": "sources", "name": "Amazon S3", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Amazon S3"}}, "data_integrations/sources/base:data_loader:sources:Amplitude:Data integration data loader block for Amplitude sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Amplitude sources.", "language": "sources", "name": "Amplitude", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Amplitude"}}, "data_integrations/sources/base:data_loader:sources:Api:Data integration data loader block for Api sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Api sources.", "language": "sources", "name": "Api", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Api"}}, "data_integrations/sources/base:data_loader:sources:Azure Blob Storage:Data integration data loader block for Azure Blob Storage sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Azure Blob Storage sources.", "language": "sources", "name": "Azure Blob Storage", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Azure Blob Storage"}}, "data_integrations/sources/base:data_loader:sources:BigQuery:Data integration data loader block for BigQuery sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for BigQuery sources.", "language": "sources", "name": "BigQuery", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "BigQuery"}}, "data_integrations/sources/base:data_loader:sources:Chargebee:Data integration data loader block for Chargebee sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Chargebee sources.", "language": "sources", "name": "Chargebee", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Chargebee"}}, "data_integrations/sources/base:data_loader:sources:Commercetools:Data integration data loader block for Commercetools sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Commercetools sources.", "language": "sources", "name": "Commercetools", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Commercetools"}}, "data_integrations/sources/base:data_loader:sources:Couchbase:Data integration data loader block for Couchbase sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Couchbase sources.", "language": "sources", "name": "Couchbase", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Couchbase"}}, "data_integrations/sources/base:data_loader:sources:Datadog:Data integration data loader block for Datadog sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Datadog sources.", "language": "sources", "name": "Datadog", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Datadog"}}, "data_integrations/sources/base:data_loader:sources:Dremio:Data integration data loader block for Dremio sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Dremio sources.", "language": "sources", "name": "Dremio", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Dremio"}}, "data_integrations/sources/base:data_loader:sources:DynamoDB:Data integration data loader block for DynamoDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for DynamoDB sources.", "language": "sources", "name": "DynamoDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "DynamoDB"}}, "data_integrations/sources/base:data_loader:sources:Facebook Ads:Data integration data loader block for Facebook Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Facebook Ads sources.", "language": "sources", "name": "Facebook Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Facebook Ads"}}, "data_integrations/sources/base:data_loader:sources:Freshdesk:Data integration data loader block for Freshdesk sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Freshdesk sources.", "language": "sources", "name": "Freshdesk", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Freshdesk"}}, "data_integrations/sources/base:data_loader:sources:Front:Data integration data loader block for Front sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Front sources.", "language": "sources", "name": "Front", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Front"}}, "data_integrations/sources/base:data_loader:sources:GitHub:Data integration data loader block for GitHub sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for GitHub sources.", "language": "sources", "name": "GitHub", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "GitHub"}}, "data_integrations/sources/base:data_loader:sources:Google Ads:Data integration data loader block for Google Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Ads sources.", "language": "sources", "name": "Google Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Ads"}}, "data_integrations/sources/base:data_loader:sources:Google Analytics:Data integration data loader block for Google Analytics sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Analytics sources.", "language": "sources", "name": "Google Analytics", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Analytics"}}, "data_integrations/sources/base:data_loader:sources:Google Search Console:Data integration data loader block for Google Search Console sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Search Console sources.", "language": "sources", "name": "Google Search Console", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Search Console"}}, "data_integrations/sources/base:data_loader:sources:Google Sheets:Data integration data loader block for Google Sheets sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Google Sheets sources.", "language": "sources", "name": "Google Sheets", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Google Sheets"}}, "data_integrations/sources/base:data_loader:sources:HubSpot:Data integration data loader block for HubSpot sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for HubSpot sources.", "language": "sources", "name": "HubSpot", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "HubSpot"}}, "data_integrations/sources/base:data_loader:sources:Intercom:Data integration data loader block for Intercom sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Intercom sources.", "language": "sources", "name": "Intercom", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Intercom"}}, "data_integrations/sources/base:data_loader:sources:Knowi:Data integration data loader block for Knowi sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Knowi sources.", "language": "sources", "name": "Knowi", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Knowi"}}, "data_integrations/sources/base:data_loader:sources:LinkedIn Ads:Data integration data loader block for LinkedIn Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for LinkedIn Ads sources.", "language": "sources", "name": "LinkedIn Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "LinkedIn Ads"}}, "data_integrations/sources/base:data_loader:sources:Microsoft SQL Server:Data integration data loader block for Microsoft SQL Server sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Microsoft SQL Server sources.", "language": "sources", "name": "Microsoft SQL Server", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Microsoft SQL Server", "uuid": "mssql"}}, "data_integrations/sources/base:data_loader:sources:Mode:Data integration data loader block for Mode sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Mode sources.", "language": "sources", "name": "Mode", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Mode"}}, "data_integrations/sources/base:data_loader:sources:Monday:Data integration data loader block for Monday sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Monday sources.", "language": "sources", "name": "Monday", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Monday"}}, "data_integrations/sources/base:data_loader:sources:MongoDB:Data integration data loader block for MongoDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for MongoDB sources.", "language": "sources", "name": "MongoDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "MongoDB"}}, "data_integrations/sources/base:data_loader:sources:MySQL:Data integration data loader block for MySQL sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for MySQL sources.", "language": "sources", "name": "MySQL", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "MySQL"}}, "data_integrations/sources/base:data_loader:sources:OracleDB:Data integration data loader block for OracleDB sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for OracleDB sources.", "language": "sources", "name": "OracleDB", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "OracleDB"}}, "data_integrations/sources/base:data_loader:sources:Outreach:Data integration data loader block for Outreach sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Outreach sources.", "language": "sources", "name": "Outreach", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Outreach"}}, "data_integrations/sources/base:data_loader:sources:Paystack:Data integration data loader block for Paystack sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Paystack sources.", "language": "sources", "name": "Paystack", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Paystack"}}, "data_integrations/sources/base:data_loader:sources:Pipedrive:Data integration data loader block for Pipedrive sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Pipedrive sources.", "language": "sources", "name": "Pipedrive", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Pipedrive"}}, "data_integrations/sources/base:data_loader:sources:PostgreSQL:Data integration data loader block for PostgreSQL sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for PostgreSQL sources.", "language": "sources", "name": "PostgreSQL", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "PostgreSQL"}}, "data_integrations/sources/base:data_loader:sources:Postmark:Data integration data loader block for Postmark sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Postmark sources.", "language": "sources", "name": "Postmark", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Postmark"}}, "data_integrations/sources/base:data_loader:sources:PowerBI:Data integration data loader block for PowerBI sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for PowerBI sources.", "language": "sources", "name": "PowerBI", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "PowerBI"}}, "data_integrations/sources/base:data_loader:sources:Redshift:Data integration data loader block for Redshift sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Redshift sources.", "language": "sources", "name": "Redshift", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Redshift"}}, "data_integrations/sources/base:data_loader:sources:Salesforce:Data integration data loader block for Salesforce sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Salesforce sources.", "language": "sources", "name": "Salesforce", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Salesforce"}}, "data_integrations/sources/base:data_loader:sources:Sftp:Data integration data loader block for Sftp sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Sftp sources.", "language": "sources", "name": "Sftp", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Sftp"}}, "data_integrations/sources/base:data_loader:sources:Snowflake:Data integration data loader block for Snowflake sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Snowflake sources.", "language": "sources", "name": "Snowflake", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Snowflake"}}, "data_integrations/sources/base:data_loader:sources:Stripe:Data integration data loader block for Stripe sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Stripe sources.", "language": "sources", "name": "Stripe", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Stripe"}}, "data_integrations/sources/base:data_loader:sources:Tableau:Data integration data loader block for Tableau sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Tableau sources.", "language": "sources", "name": "Tableau", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Tableau"}}, "data_integrations/sources/base:data_loader:sources:Twitter Ads:Data integration data loader block for Twitter Ads sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Twitter Ads sources.", "language": "sources", "name": "Twitter Ads", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Twitter Ads"}}, "data_integrations/sources/base:data_loader:sources:Zendesk:Data integration data loader block for Zendesk sources.": {"block_type": "data_loader", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data loader block for Zendesk sources.", "language": "sources", "name": "Zendesk", "path": "data_integrations/sources/base", "template_type": "data_integration", "template_variables": {"name": "Zendesk"}}, "data_integrations/destinations/base:data_exporter:destinations:Amazon S3:Data integration data exporter block for Amazon S3 destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Amazon S3 destinations.", "language": "destinations", "name": "Amazon S3", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Amazon S3"}}, "data_integrations/destinations/base:data_exporter:destinations:BigQuery:Data integration data exporter block for BigQuery destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for BigQuery destinations.", "language": "destinations", "name": "BigQuery", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "BigQuery"}}, "data_integrations/destinations/base:data_exporter:destinations:Clickhouse:Data integration data exporter block for Clickhouse destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Clickhouse destinations.", "language": "destinations", "name": "Clickhouse", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Clickhouse"}}, "data_integrations/destinations/base:data_exporter:destinations:Delta Lake Azure:Data integration data exporter block for Delta Lake Azure destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Delta Lake Azure destinations.", "language": "destinations", "name": "Delta Lake Azure", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Delta Lake Azure"}}, "data_integrations/destinations/base:data_exporter:destinations:Delta Lake S3:Data integration data exporter block for Delta Lake S3 destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Delta Lake S3 destinations.", "language": "destinations", "name": "Delta Lake S3", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Delta Lake S3"}}, "data_integrations/destinations/base:data_exporter:destinations:Elasticsearch:Data integration data exporter block for Elasticsearch destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Elasticsearch destinations.", "language": "destinations", "name": "Elasticsearch", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Elasticsearch"}}, "data_integrations/destinations/base:data_exporter:destinations:Google Cloud Storage:Data integration data exporter block for Google Cloud Storage destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Google Cloud Storage destinations.", "language": "destinations", "name": "Google Cloud Storage", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Google Cloud Storage"}}, "data_integrations/destinations/base:data_exporter:destinations:Kafka:Data integration data exporter block for Kafka destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Kafka destinations.", "language": "destinations", "name": "Kafka", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Kafka"}}, "data_integrations/destinations/base:data_exporter:destinations:MongoDB:Data integration data exporter block for MongoDB destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for MongoDB destinations.", "language": "destinations", "name": "MongoDB", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "MongoDB"}}, "data_integrations/destinations/base:data_exporter:destinations:Microsoft SQL Server:Data integration data exporter block for Microsoft SQL Server destinations (MSSQL).": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Microsoft SQL Server destinations (MSSQL).", "language": "destinations", "name": "Microsoft SQL Server", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"module_name": "MSSQL", "name": "Microsoft SQL Server", "uuid": "mssql"}}, "data_integrations/destinations/base:data_exporter:destinations:MySQL:Data integration data exporter block for MySQL destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for MySQL destinations.", "language": "destinations", "name": "MySQL", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "MySQL"}}, "data_integrations/destinations/base:data_exporter:destinations:Opensearch:Data integration data exporter block for Opensearch destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Opensearch destinations.", "language": "destinations", "name": "Opensearch", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Opensearch"}}, "data_integrations/destinations/base:data_exporter:destinations:OracleDB:Data integration data exporter block for OracleDB destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for OracleDB destinations.", "language": "destinations", "name": "OracleDB", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "OracleDB"}}, "data_integrations/destinations/base:data_exporter:destinations:PostgreSQL:Data integration data exporter block for PostgreSQL destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for PostgreSQL destinations.", "language": "destinations", "name": "PostgreSQL", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "PostgreSQL"}}, "data_integrations/destinations/base:data_exporter:destinations:Redshift:Data integration data exporter block for Redshift destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Redshift destinations.", "language": "destinations", "name": "Redshift", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Redshift"}}, "data_integrations/destinations/base:data_exporter:destinations:Salesforce:Data integration data exporter block for Salesforce destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Salesforce destinations.", "language": "destinations", "name": "Salesforce", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Salesforce"}}, "data_integrations/destinations/base:data_exporter:destinations:Snowflake:Data integration data exporter block for Snowflake destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Snowflake destinations.", "language": "destinations", "name": "Snowflake", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Snowflake"}}, "data_integrations/destinations/base:data_exporter:destinations:Trino:Data integration data exporter block for Trino destinations.": {"block_type": "data_exporter", "configuration": {"data_integration": {}}, "defaults": {"language": "yaml"}, "description": "Data integration data exporter block for Trino destinations.", "language": "destinations", "name": "Trino", "path": "data_integrations/destinations/base", "template_type": "data_integration", "template_variables": {"name": "Trino"}}}}